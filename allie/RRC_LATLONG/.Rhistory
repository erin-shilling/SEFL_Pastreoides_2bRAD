mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0))
College.recoded.Predictions %>%
select(Private, pred) %>%
table()
College.recoded.Predictions %>%
summarize(accuracy = mean(pred == Private))
Predictions <- predict(glm.multi.fit)
College.recoded.Predictions <- College.recoded %>%
mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0))
College.recoded.Predictions %>%
select(Private, pred) %>%
table()
College.recoded.Predictions %>%
summarize(accuracy = mean(pred == Private))
PLot.glm <-College.recoded %>%
ggplot(mapping = aes(x = Outstate, y = Private)) + geom_point() +
geom_smooth(method = "glm",
method.args = c(family = "binomial"))
PLot.glm
College.recoded.Predictions <- College.recoded %>%
mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0)Predictions.Multiple.Regression <- predict(glm.PrivateSchoolStatus, type = “response”))
College.recoded.Predictions <- College.recoded %>%
mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0),Predictions.Multiple.Regression <- predict(glm.PrivateSchoolStatus, type = “response”))
Predictions.Multiple.Regression <- predict(glm.PrivateSchoolStatus, type = “response”)
Predictions <- predict(glm.fit)
College.recoded.Predictions <- College.recoded %>%
mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0),
College.recoded.Predictions %>%
select(Private, pred) %>%
table()
College.recoded.Predictions %>%
summarize(accuracy = mean(pred == Private))
## Question 10
Predictions <- predict(glm.fit, TYPE=RESPPONSE)
College.recoded.Predictions <- College.recoded %>%
mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0))
College.recoded.Predictions %>%
select(Private, pred) %>%
table()
library(ISLR)
library(dplyr)
library(ggplot2)
library(modelr)
Predictions <- predict(glm.fit, TYPE=RESPPONSE)
College.recoded.Predictions <- College.recoded %>%
mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0))
College.recoded.Predictions %>%
select(Private, pred) %>%
table()
College.recoded.Predictions %>%
summarize(accuracy = mean(pred == Private))
Predictions <- predict(glm.multi.fit, TYPE=RESPONSE)
College.recoded.Predictions <- College.recoded %>%
mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0))
College.recoded.Predictions %>%
select(Private, pred) %>%
table()
College.recoded.Predictions %>%
summarize(accuracy = mean(pred == Private))
## Question 10
Predictions <- predict(glm.fit, type = Response)
## Question 10
Predictions <- predict(glm.fit, type = response)
Predictions <- predict(glm.fit, type = "response")
College.recoded.Predictions <- College.recoded %>%
mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0))
College.recoded.Predictions %>%
select(Private, pred) %>%
table()
College.recoded.Predictions %>%
summarize(accuracy = mean(pred == Private))
Predictions <- predict(glm.multi.fit, type = "response")
College.recoded.Predictions <- College.recoded %>%
mutate(ClassPredictions = Predictions, pred = ifelse(ClassPredictions > 0.5,1,0))
College.recoded.Predictions %>%
select(Private, pred) %>%
table()
College.recoded.Predictions %>%
summarize(accuracy = mean(pred == Private))
## MODULE 7&8
install.packages(glmnet)
library(glmnet)
## MODULE 7&8
install.packages(glmnet)
library(glmnet)
## MODULE 7&8
install.packages(glmnet)
## MODULE 7&8
install.packages("glmnet")
library(glmnet)
Y <- Advertising %>% select(sales) %>% as.matrix()
library(dplyr)
library(modlr)
library(modelr)
library(ggplot2)
Y <- Advertising %>% select(sales) %>% as.matrix()
X <- Advertising %>% select(TV, radio, newspaper) %>%
as.matrix()
head(Y)
head(X)
lambdas <- 10^seq(-3, 3, length.out = 100)
ridge.fit <- glmnet(X, Y, alpha = 0, lambda = lambdas)
plot(ridge.fit, xvar = "lambda")
legend("topright", lwd=1, col = 1:3, legend = colnames(X))
ridge.cv <- cv.glmnet(X, Y, alpha = 0, lambda = lambdas, nfolds = 10)
plot(ridge.cv)
ridge.best <- glmnet(X, Y, alpha = 0,
lambda = ridge.cv$lambda.min)
lm.fit
predict(ridge.best, as.matrix(tibble(TV = 1000, radio = 2000, newspaper = 3000)))
predict(lm.fit, tibble(TV = 1000, radio = 2000, newspaper = 3000))
predict(lm.fit, tibble(TV = 1000, radio = 2000, newspaper = 3000))
lasso.fit <- glmnet(X, Y, alpha = 1, lambda = lambdas)
plot(lasso.fit, xvar = "lambda")
legend("topright", lwd=1, col = 1:3, legend = colnames(X))
plot(lasso.cv)
lasso.cv <- cv.glmnet(X, Y, alpha = 1, lambda = lambdas, nfolds = 10)
plot(lasso.cv)
lasso.best <- glmnet(X, Y, alpha = 1,
lambda = lasso.cv$lambda.min)
coef(lasso.best)
coef(ridge.best)
predict(lasso.best, as.matrix(tibble(TV = 1000, radio = 2000, newspaper = 3000)))
predict(ridge.best, as.matrix(tibble(TV = 1000, radio = 2000, newspaper = 3000)))
Default.recoded <- Default %>%
mutate(default = ifelse(default == "Yes", 1, 0), student = ifelse(student == "Yes", 1, 0))
library(glmnet)
Default.recoded <- Default %>%
mutate(default = ifelse(default == "Yes", 1, 0), student = ifelse(student == "Yes", 1, 0))
library(ISLR)
Default.recoded <- Default %>%
mutate(default = ifelse(default == "Yes", 1, 0), student = ifelse(student == "Yes", 1, 0))
Y <- Default.recoded %>% select(default) %>% as.matrix()
X <- Default.recoded %>%
X <- Default.recoded %>%
head(Y)
Y <- Default.recoded %>% select(default) %>% as.matrix()
X <- Default.recoded %>%
select(balance, income, student) %>% as.matrix()
head(Y)
head(X)
lambdas <- 10^seq(-6, 6, length.out = 100)
ridge.fit <- glmnet(X, Y, family = "binomial", alpha = 0, lambda = lambdas)
plot(ridge.fit, xvar = "lambda")
legend("bottomright", lwd=1, col=1:3, legend = colnames(X))
lasso.cv <- cv.glmnet(X, Y, family = "binomial", alpha = 1, lambda = lambdas, nfolds = 10)
plot(lasso.cv)
lasso.1se <- glmnet(X, Y, family = "binomial", alpha = 1, lambda = lasso.cv$lambda.1se)
coef(lasso.1se)
coef(ridge.1se)
estProbs <- predict(lasso.1se, X, type = "response", s = lasso.cv$lambda.1se)
estClass <- predict(lasso.1se, X, type = "class", s = lasso.cv$lambda.1se)
Default.recoded.withProbs <- Default.recoded %>% mutate(probs = c(estProbs), pred = c(estClass))
head(Default.recoded.withProbs)
Default.recoded.withProbs %>% select(default, pred) %>% table()
Default.recoded.withProbs %>% summarize(accuracy = mean(pred == default))
## CAP FINAL PROJECT
## QUESTION 1
library(tidyverse)
library(glmnet)
GlobalAncestry.csv
library(GlobalAncestry.csv)
library(readxl)
GlobalAncestry <- read_excel("GlobalAncestry.csv")
View(GlobalAncestry)
library(readxl)
GlobalAncestry <- read_excel("Desktop/GlobalAncestry.csv")
View(GlobalAncestry)
GlobalAncestry <- read.csv("~/Desktop/GlobalAncestry.csv", header=FALSE)
View(GlobalAncestry)
GlobalAncestry <- read.csv("~/Desktop/GlobalAncestry.csv", header=FALSE)
View(GlobalAncestry)
head(GlobalAncestry)
GlobalAncestry <- read_csv("GlobalAncestry.csv")
anscestry
train <- select(GlobalAncestry, African,European, EastAsian, Oceanian, NativeAmerican)
train <- select(GlobalAncestry, African, European, EastAsian, Oceanian, NativeAmerican)
GlobalAncestry <- read.csv("~/Desktop/GlobalAncestry.csv", header=FALSE)
View(GlobalAncestry)
head(GlobalAncestry)
train <- select(GlobalAncestry, European, EastAsian, Oceanian, NativeAmerican)
train <- select(ancestry, African, European, EastAsian, Oceanian, NativeAmerican)
read_csv(GlobalAncestry.csv)
GlobalAncestry <- read_csv("GlobalAncestry.csv")
GlobalAncestry <- read.csv("~/Desktop/GlobalAncestry.csv", header=FALSE)
View(GlobalAncestry)
summary(GlobalAncestry)
GlobalAncestry
train <- select(GlobalAncestry, African, European, EastAsian, Oceanian, NativeAmerican)
train <- split(GlobalAncestry, African, European, EastAsian, Oceanian, NativeAmerican)
train <- split(GlobalAncestry, African, European, EastAsian, Oceanian, Native American)
train <- split(GlobalAncestry, African, European, EastAsian, Oceanian, NativeAmerican)
train <- split(GlobalAncestry, African, European, EastAsian, Oceanian)
train <-subset(GlobalAncestry %in% c("African", "European", "EastAsian", "Oceanian", "NativeAmerican"))
train <- filter(GlobalAncestry, ancestry == "African" | ancestry == "European" | ancestry == "EastAsian" | ancestry == "Oceanian" | ancestry == "NativeAmerican")
test <-filter(GlobalAncestry, ancestry=="Unknown1"|ancestry=="Unknown2"|ancestry=="Unknown3"|ancestry=="Unknown4"|ancestry=="Unknown5")
testmex <- filter(GlobalAncestry, ancestry=="Mexican")
train <- filter(GlobalAncestry, ancestry == "African" | ancestry == "European" | ancestry == "EastAsian" | ancestry == "Oceanian" | ancestry == "NativeAmerican")
test <-filter(GlobalAncestry, ancestry=="Unknown1"|ancestry=="Unknown2"|ancestry=="Unknown3"|ancestry=="Unknown4"|ancestry=="Unknown5")
testmex <- filter(GlobalAncestry, ancestry=="Mexican")
testmex <- filter(GlobalAncestry, ancestry=="Mexican")
rlang::last_error()
ancestry <- GlobalAncestry
train <- filter(GlobalAncestry, ancestry == "African" | ancestry == "European" | ancestry == "EastAsian" | ancestry == "Oceanian" | ancestry == "NativeAmerican")
test <-filter(GlobalAncestry, ancestry=="Unknown1"|ancestry=="Unknown2"|ancestry=="Unknown3"|ancestry=="Unknown4"|ancestry=="Unknown5")
testmex <- filter(GlobalAncestry, ancestry=="Mexican")
train
test
testmex
## QUESTION 3
lasso.cv <- cv.glmnet(train, alpha = 1, lambda = lambdas, nfolds = 10)
## QUESTION 3
lasso.cv <- cv.glmnet(X,Y, alpha = 1, lambda = lambdas, nfolds = 10)
library(readr)
GlobalAncestry <- read_csv("Desktop/Intro to Data Science/GlobalAncestry.csv")
View(GlobalAncestry)
install.packages("glmnet")
library(tidyverse)
library(glmnet)
library(modelr)
library(ggplot2)
Train <- filter(GlobalAncestry, ancestry == "African" | ancestry == "European" | ancestry == "EastAsian" | ancestry == "Oceanian" | ancestry == "NativeAmerican")
Test <- filter(GlobalAncestry, ancestry == "Unknown1" | ancestry == "Unknown2" | ancestry == "Unknown3" | ancestry == "Unknown4" | ancestry == "Unknown5")
Testmex <- filter(GlobalAncestry, ancestry == "Mexican")
## QUESTION 2
lambdas <- 10^seq(-3,3,length.out=100)
X <- Train %>% select (pos1:pos8916) %>% as.matrix
Y <- Train %>% select (ancestry) %>% as.matrix
lasso.fit <- glmnet(X, Y, family = "multinomial", alpha =1, lambda = lambdas )
lasso.plot <- plot(lasso.fit, xvar = "lambda")
lasso.cv <- cv.glmnet(X,Y, family = "multinomial",alpha = 1, lambda = lambdas, nfolds = 10)
plot(lasso.cv)
lasso.cv$lambda.1se
lasso.cv$lambda.min
lasso.1se <- glmnet(X, Y,family = "multinomial", alpha = 1, lambda = lasso.cv$lambda.1se)
lasso.1se
coef(lasso.1se)
## QUESTION 3
lasso.cv <- cv.glmnet(X,Y, family = "multinomial",alpha = 1, lambda = lambdas, nfolds = 10)
plot(lasso.cv)
lasso.cv$lambda.1se
lasso.cv$lambda.min
lasso.1se <- glmnet(X, Y,family = "multinomial", alpha = 1, lambda = lasso.cv$lambda.1se)
lasso.1se
coef(lasso.1se)
## QUESTION 4
estClass <- predict(lasso.1se, X, type = "class", s= lasso.cv$lambda.1se)
train.withPreds <- Train %>%
mutate(probs =c(estClass))
head(train.withPreds)
train.withPreds %>% select(ancestry, probs) %>%
table()
train.withPreds %>% summarize(accuracy = mean(probs == ancestry))
## QUESTION 5
TestX <- Test %>% select(pos1:pos8916) %>% as.matrix
TestY <- Test %>% select(ancestry) %>% as.matrix
estAncestry <- predict(lasso.1se, TestX, type = "class", s=lasso.cv$lambda.1se)
estAncestry
Test %>%
mutate(pred=c(estAncestry)) %>%
select(ancestry,pred) %>%
arrange(ancestry)
PredictedAncestry <- Test %>%
mutate(probs1=c(estAncestry))
head(PredictedAncestry)
PredictedAncestry %>% select(ancestry,probs1) %>%
table()
## QUESTION 6
MexX <- Testmex %>% select(pos1:pos8916) %>% as.matrix
MexY <- Testmex %>% select(ancestry) %>% as.matrix
estMexican <- predict(lasso.1se, MexX, type ="response", s=lasso.cv$lambda.1se)
#responses will be the probabilities rather than the classes
as_tibble(estMexican) %>%
ggplot() +
geom_violin(aes(x= "African", y=African.1), fill= "orange") +
geom_violin(aes(x="European", y=European.1), fill="blue") +
geom_violin(aes(x="EastAsian", y=EastAsian.1), fill="pink") +
geom_violin(aes(x="Oceanian", y=Oceanian.1), fill ="green") +
geom_violin(aes(x="NativeAmerican", y=NativeAmerican.1), fill="purple")
install.packages('readxl')
library("read_excel")
install.packages("readxl")
library("read_excel")
library("readxl")
sampleorder= read_excel("~/Desktop/SampleOrder.xlsx")
View(sampleorder)
rrcMetaDataBW = read_excel("~/Desktop/RRC_sample_metadata_v2.xlsx")
install.packages("tidyverse")
library("tidyverse")
newMetaData = sampleorder %>% left_join(rrcMetaDataBW, by = "Colony.ID")
newMetaData = sampleorder %>% left_join(rrcMetaDataBW, by = "Colony ID")
write_csv(newMetaData, file = "updatedRrcMetaData.csv")
View(newMetaData)
View(newMetaData)
View(newMetaData)
View(newMetaData)
newMetaDataAK = sampleorder %>% left_join(rrcMetaDataBW, by = "Colony ID")
write_csv(newMetaDataAK, file = "updatedRrcMetaDataAK.csv")
plot_bayescan<-function(res,FDR=0.05,size=1,pos=0.35,highlight=NULL,name_highlighted=F,add_text=T)
{
if (is.character(res))
res=read.table(res)
colfstat=5
colq=colfstat-2
highlight_rows=which(is.element(as.numeric(row.names(res)),highlight))
non_highlight_rows=setdiff(1:nrow(res),highlight_rows)
outliers=as.integer(row.names(res[res[,colq]<=FDR,]))
ok_outliers=TRUE
if (sum(res[,colq]<=FDR)==0)
ok_outliers=FALSE;
res[res[,colq]<=0.0001,colq]=0.0001
# plot
plot(log10(res[,colq]),res[,colfstat],xlim=rev(range(log10(res[,colq]))),xlab="log10(q value)",ylab=names(res[colfstat]),type="n")
points(log10(res[non_highlight_rows,colq]),res[non_highlight_rows,colfstat],pch=19,cex=size)
if (name_highlighted) {
if (length(highlight_rows)>0) {
text(log10(res[highlight_rows,colq]),res[highlight_rows,colfstat],row.names(res[highlight_rows,]),col="red",cex=size*1.2,font=2)
}
}
else {
points(log10(res[highlight_rows,colq]),res[highlight_rows,colfstat],col="red",pch=19,cex=size)
# add names of loci over p and vertical line
if (ok_outliers & add_text) {
text(log10(res[res[,colq]<=FDR,][,colq])+pos*(round(runif(nrow(res[res[,colq]<=FDR,]),1,2))*2-3),res[res[,colq]<=FDR,][,colfstat],row.names(res[res[,colq]<=FDR,]),cex=size)
}
}
lines(c(log10(FDR),log10(FDR)),c(-1,1),lwd=1, lty=3)
return(list("outliers"=outliers,"nb_outliers"=length(outliers)))
}
plot_bayescan<-function(res,FDR=0.05,size=1,pos=0.35,highlight=NULL,name_highlighted=F,add_text=T)
{
if (is.character(res))
res=read.table(res)
colfstat=5
colq=colfstat-2
highlight_rows=which(is.element(as.numeric(row.names(res)),highlight))
non_highlight_rows=setdiff(1:nrow(res),highlight_rows)
outliers=as.integer(row.names(res[res[,colq]<=FDR,]))
ok_outliers=TRUE
if (sum(res[,colq]<=FDR)==0)
ok_outliers=FALSE;
res[res[,colq]<=0.0001,colq]=0.0001
# plot
plot(log10(res[,colq]),res[,colfstat],xlim=rev(range(log10(res[,colq]))),xlab="log10(q value)",ylab=names(res[colfstat]),type="n")
points(log10(res[non_highlight_rows,colq]),res[non_highlight_rows,colfstat],pch=19,cex=size)
if (name_highlighted) {
if (length(highlight_rows)>0) {
text(log10(res[highlight_rows,colq]),res[highlight_rows,colfstat],row.names(res[highlight_rows,]),col="red",cex=size*1.2,font=2)
}
}
else {
points(log10(res[highlight_rows,colq]),res[highlight_rows,colfstat],col="red",pch=19,cex=size)
# add names of loci over p and vertical line
if (ok_outliers & add_text) {
text(log10(res[res[,colq]<=FDR,][,colq])+pos*(round(runif(nrow(res[res[,colq]<=FDR,]),1,2))*2-3),res[res[,colq]<=FDR,][,colfstat],row.names(res[res[,colq]<=FDR,]),cex=size)
}
}
lines(c(log10(FDR),log10(FDR)),c(-1,1),lwd=1, lty=3)
return(list("outliers"=outliers,"nb_outliers"=length(outliers)))
}
# 	 This file is used to plot figures for the software Bayescan in R.
#    This program, BayeScan, aims at detecting genetics markers under selection,
#	 based on allele frequency differences between population.
#    Copyright (C) 2010  Matthieu Foll
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
# Arguments:
# - file is the name of your file ex: "output_fst.txt"
# - the q-value threshold corresponding to the target False Discovery Rate (FDR)
# - size is the size of the points and text labels for outliers
# - pos is the distance between the points and the labels
# - highlight is a optional list of marker indices to display in red.
# - name_highlighted alows to write the indices of highlighted markers instead of using a point like the other markers
# - add_text adds the indices of the outlier markers
# Output:
# This function returns different paremeters in a list
# - outliers: the list of outliers
# - nb_outliers: the number of outliers
# Typical usage:
# - load this file into R (file/source R code)
# - in R, go to the directory where "output_fst.txt" is (file/change current dir)
# - at the R prompt, type
# > plot_bayescan("output_fst.txt",0,FDR=0.05)
# if you save the output in a variable, you can recall the different results:
# results<-plot_bayescan("output_fst.txt",0,FDR=0.05)
# results$outliers
# results$nb_outliers
#
# plotting posterior distribution is very easy in R with the output of BayeScan:
# first load the output file *.sel produced by BayeScan
# > mydata=read.table("bi.sel",colClasses="numeric")
# choose the parameter you want to plot by setting for example:
# parameter="Fst1"
# then this line will make the plot for:
# > plot(density(mydata[[parameter]]),xlab=parameter,main=paste(parameter,"posterior distribution"))
# you can plot population specific Fst coefficient by setting
# parameter="Fst1"
# if you have non-codominant data you can plot posterior for Fis coefficients in each population:
# parameter="Fis1"
# if you test for selection, you can plot the posterior for alpha coefficient for selection:
# parameter="alpha1"
# you also have access to the likelihood with:
# parameter="logL"
# if you have the package "boa" installed, you can very easily obtain Highest Probability
# Density Interval (HPDI) for your parameter of interest (example for the 95% interval):
# > boa.hpd(mydata[[parameter]],0.05)
plot_bayescan<-function(res,FDR=0.05,size=1,pos=0.35,highlight=NULL,name_highlighted=F,add_text=T)
{
if (is.character(res))
res=read.table(res)
colfstat=5
colq=colfstat-2
highlight_rows=which(is.element(as.numeric(row.names(res)),highlight))
non_highlight_rows=setdiff(1:nrow(res),highlight_rows)
outliers=as.integer(row.names(res[res[,colq]<=FDR,]))
ok_outliers=TRUE
if (sum(res[,colq]<=FDR)==0)
ok_outliers=FALSE;
res[res[,colq]<=0.0001,colq]=0.0001
# plot
plot(log10(res[,colq]),res[,colfstat],xlim=rev(range(log10(res[,colq]))),xlab="log10(q value)",ylab=names(res[colfstat]),type="n")
points(log10(res[non_highlight_rows,colq]),res[non_highlight_rows,colfstat],pch=19,cex=size)
if (name_highlighted) {
if (length(highlight_rows)>0) {
text(log10(res[highlight_rows,colq]),res[highlight_rows,colfstat],row.names(res[highlight_rows,]),col="red",cex=size*1.2,font=2)
}
}
else {
points(log10(res[highlight_rows,colq]),res[highlight_rows,colfstat],col="red",pch=19,cex=size)
# add names of loci over p and vertical line
if (ok_outliers & add_text) {
text(log10(res[res[,colq]<=FDR,][,colq])+pos*(round(runif(nrow(res[res[,colq]<=FDR,]),1,2))*2-3),res[res[,colq]<=FDR,][,colfstat],row.names(res[res[,colq]<=FDR,]),cex=size)
}
}
lines(c(log10(FDR),log10(FDR)),c(-1,1),lwd=1, lty=3)
return(list("outliers"=outliers,"nb_outliers"=length(outliers)))
}
outliers
library(tidyverse)
library(lubridate)
library(measurements)
library(sf)
install.packages("sf")
library(sf)
install.packages("rnaturalearth")
library(sf)
library(rnaturalearth)
library(tidyverse)
library(lubridate)
library(measurements)
library(sf)
install.packages("pacman")
library(pacman)
setwd("~/Documents/GitHub/RRCOrbicella/OfavKoko/RRC_LATLONG")
ofavSamples = read.csv("updatedRRC_LatLong.csv", header = TRUE) %>% select(sample = Sample, !sampleID)
library(pacman)
setwd("~/Documents/GitHub/RRCOrbicella/OfavKoko/RRC_LATLONG")
ofavSamples = read.csv("updatedRRC_LatLong.csv", header = TRUE) %>% select(sample = Sample, !sampleID)
ofavSamples$Resistance = factor(ofavSamples$Resistance, levels = levels(ofavSamples$Resistance)[c(3,2,1)])
ofavSamples$Site = factor(ofavSamples$Site, levels = levels(ofavSamples$Site)[c(5, 4, 1, 3, 2)])
ofavPops = ofavSamples %>% group_by(Site) %>% summarize(latDD = mean(latDD), longDD = mean(longDD), n = n()) %>% droplevels()
ofavPops = ofavSamples %>% group_by(Site) %>% summarize(latDD = mean(latDD), longDD = mean(lonDD), n = n()) %>% droplevels()
## `summarise()` ungrouping output (override with `.groups` argument)
ofavSampleSites = ofavSamples %>% group_by(Site, Resistance) %>% summarize(latDD = min(latDD), longDD = min(lonDD))
#ofavSamples$collection_date = mdy(ofavSamples$collection_date) %>% format("%d %b %Y")
#ofavSamples$depthM = conv_unit(ofavSamples$collection_depth_ft, from = "ft", to = "m") %>% round(1)
seflSites = ofavSamples %>% group_by(Site)%>% summarize(latDD = first(latDD), longDD = first(lonDD)) %>% droplevels() %>% as.data.frame()
ofavSamples = read.csv("updatedRRC_LatLong.csv", header = TRUE) %>% select(sample = Sample, !sampleID)
ofavSamples$Resistance = factor(ofavSamples$Resistance, levels = levels(ofavSamples$Resistance)[c(3,2,1)])
ofavSamples$Site = factor(ofavSamples$Site, levels = levels(ofavSamples$Site)[c(5, 4, 1, 3, 2)])
ofavPops = ofavSamples %>% group_by(Site) %>% summarize(latDD = mean(latDD), longDD = mean(lonDD), n = n()) %>% droplevels()
## `summarise()` ungrouping output (override with `.groups` argument)
ofavSampleSites = ofavSamples %>% group_by(Site, Resistance) %>% summarize(latDD = min(latDD), longDD = min(lonDD))
states = st_as_sf(ne_states(country = c("United States of America")))
countries = st_as_sf(ne_countries(country = c("Cuba", "The Bahamas")))
library(pacman)
library(rnaturalearth)
library(sf)
install.packages("rnaturalearth")
library(rnaturalearth)
install.packages("sf")
library(sf)
setwd("~/Documents/GitHub/RRCOrbicella/OfavKoko/RRC_LATLONG")
ofavSamples = read.csv("updatedRRC_LatLong.csv", header = TRUE) %>% select(sample = Sample, !sampleID)
ofavSamples$Resistance = factor(ofavSamples$Resistance, levels = levels(ofavSamples$Resistance)[c(3,2,1)])
ofavSamples$Site = factor(ofavSamples$Site, levels = levels(ofavSamples$Site)[c(5, 4, 1, 3, 2)])
ofavPops = ofavSamples %>% group_by(Site) %>% summarize(latDD = mean(latDD), longDD = mean(lonDD), n = n()) %>% droplevels()
## `summarise()` ungrouping output (override with `.groups` argument)
ofavSampleSites = ofavSamples %>% group_by(Site, Resistance) %>% summarize(latDD = min(latDD), longDD = min(lonDD))
states = st_as_sf(ne_states(country = c("United States of America")))
library(lubridate)
library(measurements)
ofavSamples = read.csv("updatedRRC_LatLong.csv", header = TRUE) %>% select(sample = Sample, !sampleID)
ofavSamples$Resistance = factor(ofavSamples$Resistance, levels = levels(ofavSamples$Resistance)[c(3,2,1)])
ofavSamples$Site = factor(ofavSamples$Site, levels = levels(ofavSamples$Site)[c(5, 4, 1, 3, 2)])
ofavPops = ofavSamples %>% group_by(Site) %>% summarize(latDD = mean(latDD), longDD = mean(lonDD), n = n()) %>% droplevels()
## `summarise()` ungrouping output (override with `.groups` argument)
ofavSampleSites = ofavSamples %>% group_by(Site, Resistance) %>% summarize(latDD = min(latDD), longDD = min(lonDD))
states = st_as_sf(ne_states(country = c("United States of America")))
install.packages("sf")
install.packages("rnaturalearth")
library(sf)
