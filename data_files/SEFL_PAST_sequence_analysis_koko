#SEFL Porites astreoides 2bRAD analysis
#I made a directory "2bRAD_PAST_SEFL" in koko & downloaded the raw sequences into it using WinSCP
#Using Ryan's script/protocol for his Stephanocoenia intersepta de novo 2bRAD sequence alignment

#login using PUTTY
eshilling2013@koko-login.hpc.fau.edu
Smilebigz1!

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#pause actions running
ctrl + z

#make a program executable
#chmod = change mode; +x means make executable, -x means opposite
chmod +x ~/bin/filename

#remove a file; rm is remove, -r is telling to get rid of the directory & everything in it
rm -r "file or directory name"

#to make something run in background
% (at the end)

#to assign to not the head node, make things run faster/not clog it up
srun

#to copy a file
cp ../thedirectoryyou'recopyingthefilefrom/filenametocopy .
#example
cp ../ANGSD/pastNoClones.bcf .

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

S E T U P
Download all necessary scripts and load modules for processing/analysis

Load necessary modules
or you can add to ~/.bashrc to load at login (use nano .bashrc)

module load angsd-0.933-gcc-9.2.0-65d64pp
module load bayescan-2.1-gcc-8.3.0-7gakqmd
module load qt-5.15.2-gcc-9.2.0-zi7wcem BayeScEnv/1.1
module load bcftools-1.9-gcc-8.3.0-il4d373
module load bowtie2-2.3.5.1-gcc-8.3.0-63cvhw5
module load cdhit-4.8.1-gcc-8.3.0-bcay75d
module load htslib-1.9-gcc-8.3.0-jn7ehrc
module load kraken2-2.1.1-gcc-9.2.0-ocivj3u
module load python-3.7.4-gcc-8.3.0-3tniqr5
module load launcher
module load miniconda3-4.6.14-gcc-8.3.0-eenl5dj
module load ncbi-toolkit-22_0_0-gcc-9.2.0-jjhd2wa
module load ngsadmix-32-gcc-8.3.0-qbnwmpq
module load ngsRelate/v2
module load R/3.6.1
module load samtools-1.10-gcc-8.3.0-khgksad
module load vcftools-0.1.14-gcc-8.3.0-safy5vc

#Download scripts
#Put scripts needed into ~/bin
cd ~/bin
svn checkout https://github.com/RyanEckert/Stephanocoenia_FKNMS_PopGen/trunk/scripts/
mv scripts/* .
rm -r scripts

#download another program for converting file formats
wget http://www.cmpg.unibe.ch/software/PGDSpider/PGDSpider_2.0.7.1.zip
unzip PGDSpider_2.0.7.1.zip
rm PGDSpider_2.0.7.1.zip

#Make all scripts executable
chmod +x *.sh *.pl *.py *.R *.r
#check that they are all in the folder & executable

Build working directory
mkdir 2bRAD_PAST_SEFL
mkdir 2bRAD_PAST_SEFL_/rawReads/

##downloading raw reads from Base Space, unzipping, then merging/concatenating, then rezipping them into new directory concatReads
wget "https://launch.basespace.illumina.com/CLI/latest/amd64-linux/bs" -O $HOME/bin/bs

echo '#!/bin/bash' > downloadReads.sh
echo 'bs download project --concurrency=high -q -n JA21114 -o .' >> downloadReads.sh
# -n is the project name and -o is the output directory

echo "find . -name '*.gz' -exec mv {} . \;" >> downloadReads.sh
echo 'rmdir SA*' >>downloadReads.sh
echo 'mkdir ../concatReads' >> downloadReads.sh
echo 'cp *.gz ../concatReads' >> downloadReads.sh
echo 'cd ../concatReads' >> downloadReads.sh
echo 'mergeReads.sh -o mergeTemp' >> downloadReads.sh
# -o is the directory to put output files in

echo 'rm *L00*' >> downloadReads.sh
echo "find . -name '*.gz' -exec mv {} . \;" >> downloadReads.sh
echo 'gunzip *.gz' >> downloadReads.sh
echo 'rmdir mergeTemp' >> downloadReads.sh

chmod +x downloadReads.sh

launcher_creator.py -b 'srun downloadReads.sh' -n downloadReads -q shortq7 -t 06:00:00 -e eshilling2013@fau.edu
sbatch downloadReads.slurm

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

How many reads before filtering?

cd concatReads
echo '#!/bin/bash' >rawReads
echo ~/bin/readCounts.sh -e .fastq -o pastRaw >>rawReads

sbatch -o rawReads.o%j -e rawReads.e%j rawReads --mail-type=ALL --mail-user=eshilling2013@fau.edu

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
T R I M M I N G   &   F I L T E R I N G
Trim and demultiplex reads
cd ../concatReads

##having issues at this step, may try running on a medium node instead of short?
2bRAD_trim_launch_dedup.pl fastq > trims.sh
launcher_creator.py -j trims.sh -n trims -q shortq7 -t 06:00:00 -e eshilling2013@fau.edu
sbatch --mem=200GB trims.slurm
Check that we have the correct number of trim files (96 in this case)

ls -l *.tr0 | wc -l

mkdir ../trimmedReads
srun mv *.tr0 ../trimmedReads &

zipper.py -f fastq -a -9 --launcher -e eshilling2013@fau.edu
sbatch --mem=200GB zip.slurm

cd ../trimmedReads
Rename sequence files using sampleRename.py This script needs a .csv with XXX Make sure you use the reverse complement of your inline BCs!

srun sampleRename.py -i sampleListR -f tr0
Quality filtering using cutadapt
I can’t get KoKo’s module for cutadapt to work, so we’ll do it in miniconda. Run below if you don’t have a conda env. set up, otherwise you can skip to the next chunk

module load miniconda3-4.6.14-gcc-8.3.0-eenl5dj
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda create -n 2bRAD cutadapt
#Removing reads with qualities at ends less than Q15 for de novo analysis

source activate 2bRAD

echo '#!/bin/bash' > trimse.sh
echo 'module load miniconda3-4.6.14-gcc-8.3.0-eenl5dj' >> trimse.sh
echo 'source activate 2bRAD' >> trimse.sh
for file in *.tr0; do
echo "cutadapt -q 15,15 -m 36 -o ${file/.tr0/}.trim $file > ${file/.tr0/}.trimlog.txt" >> trimse.sh;
done
#I can’t get it to run through launcher so just run it serially, it takes a while to run, so consider breaking up in several jobs.

sbatch -o trimse.o%j -e trimse.e%j --mem=200GB trimse.sh
Do we have expected number of *.trim files created?

conda deactivate 2bRAD
ls -l *.trim | wc -l
How many reads in each sample?

echo '#!/bin/bash' >pastReads
echo readCounts.sh -e trim -o pastFilt >>pastReads
sbatch --mem=200GB pastReads

mkdir ../filteredReads
mv *.trim ../filteredReads

zipper.py -f tr0 -a -9 --launcher -e eshilling2013@fau.edu
sbatch zip.slurm

cat pastFiltReadCounts
#output
P001    468751
P002    111637
P003    849170
P004    1879703
P005    700388
P006    1148154
P007    672108
P008    825448
P009    1599187
P010    570287
P011    789515
P012    847606
P013    524527
P014    1124516
P015    565903
P018    1955399
P019    1131834
P020    543463
P021    1379041
P022    2310556
P023    1631768
P024    404739
P025    1383589
P026    1140880
P027    1136394
P028-1  1186391
P028-2  3187198
P028-3  1808452
P029    580711
P030    1149397
P031    1023692
P032    920304
P033    2295599
P034    536858
P035    1005668
P036    1129564
P037    749988
P038    1201063
P039    149216
P040    435657
P042    386023
P043    1615020
P044    2059262
P045    1368326
P046-1  1594445
P046-2  2317465
P046-3  1508966
P047    886168
P048    1534995
P049    2518885
P050    759433
P051    854478
P052    998491
P053    567449
P054    1046525
P055    867783
P056    952207
P057    3658952
P058    1789376
P059    857766
P060    1106806
P061    1313539
P062    1544899
P063    1215034
P064    1083956
P065    1719292
P066    1994001
P067    2339818
P068    2934519
P069    1303248
P070    1442997
P071    1306903
P072    1387027
P073    3448107
P074    2420199
P075    2114350
P076    1909569
P077-1  1620282
P077-2  1251545
P077-3  2951380
P078    1757002
P079    1652323
P080    1667269
P081    4402705
P082    4148247
P083    4035593
P084    3891028
P085    3013841
P086    2668605
P087    2819284
P088    3045169
P089    4212996
P090    2848035
P1-5_AGGG       296119
P1-5_TCCG       174101


D E N O V O   R E F E R E N C E
Construct denovo reference for aligning reads

Remove symbiodiniaceae reads
If you’ve not already, build a bt reference for the concatenated zoox genomes, otherwise skip ahead to the next chunk

mkdir ~/bin/symGenomes
cd ~/bin/symGenomes
echo "bowtie2-build symbConcatGenome.fasta symbConcatGenome" > bowtie2-build
launcher_creator.py -j bowtie2-build -n bowtie2-build -q shortq7 -t 06:00:00 -e eshilling2013@fau.edu
module load bowtie2-2.3.5.1-gcc-8.3.0-63cvhw5
sbatch --mem=200GB bowtie2-build.slurm
module load samtools-1.10-gcc-8.3.0-khgksad
srun samtools faidx symbConcatGenome.fasta

#Mapping reads to concatenated Symbiodinaceae genome

#make sure in filtered reads directory
mkdir symbionts
SYMGENOME=~/bin/symGenomes/symbConcatGenome

2bRAD_bowtie2_launcher.py -g $SYMGENOME -f .trim -n zooxMaps --split -u un -a zoox --aldir symbionts --launcher -e eshilling2013@fau.edu

sbatch --mem=200GB zooxMaps.slurm
Checking Symbiodiniaceae read mapping rates

ls *trim | cut -d '.' -f 1 >align1
grep "% overall" zooxMaps.e* | cut -d ' ' -f 1 >align2
paste <(awk -F' ' '{print $1}' align1) <(awk -F' ' '{print $1}' align2) >zooxAlignmentRates
rm align1 align2

less zooxAlignmentRates
P001    6.42%
P002    6.12%
P003    1.07%
P004    2.25%
P005    %%
P006    4.87%
P007    1.38%
P008    2.12%
P009    1.651.60%%
P010    1.94%
P011    2.17%
P012    3.68%
P013    1.50%
P014    1.90%
P015    1.46%
P018    5.50%
P019    1.91%
P020    1.66%
P021    2.43%
P022    1.95%
P023    2.45%
P024    1.56%
P025    5.69%
P026    3.14%
P027    1.53%
P028-1  2.04%
P028-2  1.93%
P028-3  2.69%
P029    2.26%
P030    2.34%
P031    1.80%
P032    1.08%
P033    1.92%
P034    2.01%
P035    2.38%
P036    1.42%
P037    2.09%
P038    2.19%
P039    0.62%
P040    1.34%
P042    1.98%
P043    1.70%
P044    2.67%
zooxAlignmentRates

Clean up directory

rm *.sam

zipper.py -f .trim -a -9 --launcher -e eshilling2013@fau.edu
sbatch --mem=200GB zip.slurm

#Uniquing reads
#‘stacking’ individual trimmed fastq reads:

ls *.trim.un | perl -pe 's/^(.+)$/uniquerOne.pl $1 >$1\.uni/' > unique

launcher_creator.py -j unique -n unique -q shortq7 -t 06:00:00 -e eshilling2013@fau.edu
sbatch --mem=200GB unique.slurm

#Checking there is a .uni for all samples

ls -l *.uni | wc -l

#Collecting common tags (major alleles).
#Merging uniqued files (set minInd to >10, or >10% of total number of samples, whichever is greater).

echo 'mergeUniq.pl uni minInd=10 > all.uniq' > allunique

launcher_creator.py -j allunique -n allunique -q shortq7 -t 06:00:00 -e eshilling2013@fau.edu
sbatch --mem=200GB allunique.slurm

#Discarding tags that have more than 7 observations without reverse-complement

srun awk '!($3>7 && $4==0) && $2!="seq"' all.uniq >all.tab

#Creating fasta file out of merged and filtered tags:

srun awk '{print ">"$1"\n"$2}' all.tab >all.fasta

#Clustering allowing for up to 3 mismatches (-c 0.91); the most abundant sequence becomes reference

echo '#!/bin/bash' >cdhit
echo cd-hit-est -i all.fasta -o cdh_alltags.fas -aL 1 -aS 1 -g 1 -c 0.91 -M 0 -T 0 >>cdhit
sbatch --mem=200GB -e cdhit.e%j -o cdhit.o%j cdhit

rm *.uni

#Remove contamination
#We can remove contamination sequences from our denovo reference with kraken

#We need a Kraken database to search against. If you don’t already have one, you need to build one now. otherwise you can skip to running Kraken
#We can download a compiled standard database:
##using Ryan's koko, he already has this

mkdir ~/bin/krakenDB
cd ~/bin/krakenDB

srun wget https://genome-idx.s3.amazonaws.com/kraken/k2_pluspf_20210127.tar.gz
echo tar -xvf k2_pluspf_20210127.tar.gz >tar

launcher_creator.py -j tar -n tar -q mediumq7 -t 24:00:00 -e reckert2017@fau.edu

sbatch tar.slurm

#Alternatively, we can build a custom database, which can include the Symbiodiniaceae genomes

echo '#!/bin/bash' >krakendb.sh
echo kraken2-build --download-taxonomy --db ~/bin/krakenDB >>>krakendb.sh
echo kraken2-build --download-library archaea --threads 16 --db ~/bin/krakenDB >>krakendb.sh
echo kraken2-build --download-library bacteria --threads 16 --db ~/bin/krakenDB >>krakendb.sh
echo kraken2-build --download-library viral --threads 16 --db ~/bin/krakenDB >>krakendb.sh
echo kraken2-build --download-library human --threads 16 --db ~/bin/krakenDB >>krakendb.sh
echo kraken2-build --download-library fungi --threads 16 --db ~/bin/krakenDB >>krakendb.sh
echo kraken2-build --download-library protozoa --threads 16 --db ~/bin/krakenDB >>krakendb.sh
echo kraken2-build --download-library UniVec_Core --threads 16 --db ~/bin/krakenDB >>krakendb.sh

sbatch --mem=200GB -p longq7 -e krakenDB.e%j -o krakenDB.o%j krakendb.sh

#Format and add Symbiodiniaceae genomes to the database

cd ~/bin/symGenomes

# Symbiodinium microadriaticum
sed '/>/ s/$/|kraken:taxid|2951/' Symbiodinium_microadriacticum_genome.scaffold.fasta >S_microadriacticum.fa

# Breviolum minutum
sed '/>/ s/$/|kraken:taxid|2499525/' Breviolum_minutum.v1.0.genome.fa >B_minutum.fa

# Cladocopium goreaui
sed '/>/ s/$/|kraken:taxid|2562237/' Cladocopium_goreaui_Genome.Scaffolds.fasta >C_goreaui.fa

# Durusdinium trenchii
sed '/>/ s/$/|kraken:taxid|1381693/' 102_symbd_genome_scaffold.fa >D_trenchii.fa

echo '#!/bin/bash' >kdbAdd
echo kraken2-build --add-to-library ~/bin/symGenomes/S_microadriacticum.fa --db ~/bin/krakenDB >>kdbAdd
echo kraken2-build --add-to-library ~/bin/symGenomes/B_minutum.fa --db ~/bin/krakenDB >>kdbAdd
echo kraken2-build --add-to-library ~/bin/symGenomes/C_goreaui.fa --db ~/bin/krakenDB >>kdbAdd
echo kraken2-build --add-to-library ~/bin/symGenomes/D_trenchii.fa --db ~/bin/krakenDB >>kdbAdd

sbatch --mem=200GB -o kdbAdd.o%j -e kdbAdd.e%j kdbAdd

#Finally, build the database

echo '#!/bin/bash' >kdbBuild
echo kraken2-build --build --db ~/bin/krakenDB >>kdbBuild
sbatch --mem=200GB -o kdbBuild.o%j -e kdbBuild.e%j kdbBuild

#Remove potential contamination from reference

cd ~/2bRAD/past/sefl/filteredReads

echo '#!/bin/bash' >krakenDB
echo kraken2 --db ~/bin/krakenDB cdh_alltags.fas --threads 16 --classified-out cdh_alltags.contam.fa --unclassified-out cdh_alltags.unclass.fa --report krakenDB.report --output krakenDB.out >>krakenDB

sbatch --mem=200GB -o krakenDB.o%j -e krakenDB.e%j krakenDB

#Additionally, I use a kraken blastNT database to remove additional sequences not mapping to Cnidarian taxa.

echo '#!/bin/bash' >krakenNT
echo kraken2 --db ~/bin/krakenNT/nt_db cdh_alltags.unclass.fa --threads 16 --classified-out cdh_alltags.class.fa --unclassified-out past_denovo.fa --report krakenNT.report --output krakenNT.out >>krakenNT

sbatch -o krakenNT.o%j -e krakenNT.e%j krakenNT

extract_kraken_reads.py -s cdh_alltags.class.fa -k krakenNT.out -r krakenNT.report -t 6073 --include-children -o past_denovo.fa --append

#Construct denovo genome
#With 30 pseudo chromosomes from clean major allele tags

mkdir ../mappedReads
mv past_denovo.fa ../mappedReads
cd ../mappedReads

concatFasta.pl fasta=past_denovo.fa num=30

>9300 records per chromosome

#Format pseudo genome

GENOME_FASTA=past_denovo_cc.fasta

echo '#!/bin/bash' >genomeBuild.sh
echo bowtie2-build $GENOME_FASTA $GENOME_FASTA >>genomeBuild.sh
echo samtools faidx $GENOME_FASTA >>genomeBuild.sh

sbatch -o genomeBuild.o%j -e genomeBuild.e%j --mem=200GB genomeBuild.sh

M A P P I N G   R E A D S   T O   R E F E R E N C E
Mapping reads to reference and formatting bam files

Map reads to fake genome:

mv ../filteredReads/*.un .
mv ../filteredReads/symbionts .

GENOME_FASTA=past_denovo_cc.fasta

# mapping with --local option, enables clipping of mismatching ends (guards against deletions near ends of RAD tags)
2bRAD_bowtie2_launcher.py -f un -g $GENOME_FASTA --launcher -e eshilling2013@fau.edu
sbatch --mem=200GB maps.slurm

#Do we have the right number of SAM files?

ls *.sam | wc -l

#>yes, 95

#Checking alignment rates

ls *un | cut -d '.' -f 1 >align1
grep "% overall" maps.e* | cut -d ' ' -f 1 >align2
>alignmentRates
paste <(awk -F' ' '{print $1}' align1) <(awk -F' ' '{print $1}' align2) >alignmentRates
rm align1 align2

less alignmentRates

#Convert SAM files to BAM files
#BAM files will be used for genotyping, population structure, etc.

>s2b
for file in *.sam; do
echo "samtools sort -O bam -o ${file/.sam/}.bam $file && samtools index ${file/.sam/}.bam">>s2b;
done

launcher_creator.py -j s2b -n s2b -q shortq7 -t 06:00:00 -e eshilling2013@fau.edu
sbatch --mem=200GB s2b.slurm

#Do we have enough BAM files?

ls *bam | wc -l  # should be the same number as number of trim files
#>yes, 95

#Clean up directory

zipper.py -a -9 -f sam --launcher -e eshilling2013@fau.edu
sbatch zip.slurm

rm *.un

G E N O T Y P I N G
“FUZZY genotyping” with ANGSD - without calling actual genotypes but working with genotype likelihoods at each SNP. Optimal for low-coverage data (<10x).

mkdir ../ANGSD
cd ../ANGSD
mv ../mappedReads/*.bam* .

ls *bam >bamsClones

#Assessing base qualities and coverage depth
#ANGSD settings: -minMapQ 20: only highly unique mappings (prob of erroneous mapping =< 1%) -baq 1: realign around indels (not terribly relevant for 2bRAD reads mapped with –local option) -maxDepth: highest total depth (sum over all samples) to assess; set to 10x number of samples -minInd: the minimal number of individuals the site must be genotyped in. Reset to 50% of total N at this stage.

export FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -maxDepth 960 -minInd 47"
export TODO="-doQsDist 1 -doDepth 1 -doCounts 1 -dumpCounts 2"

echo '#!/bin/bash' >pastDD.sh
echo angsd -b bamsClones -GL 1 $FILTERS $TODO -P 1 -out dd >>pastDD.sh

sbatch --mem=200GB -o pastDD.o%j -e pastDD.e%j --mail-user=eshilling2013@fau.edu --mail-type=ALL pastDD.sh

#Summarizing results (using Misha Matz modified script by Matteo Fumagalli)

echo '#!/bin/bash' >RQC.sh
echo Rscript ~/bin/plotQC.R prefix=dd >>RQC.sh
echo gzip -9 dd.counts >>RQC.sh
sbatch -e RQC.e%j -o RQC.o%j --dependency=afterok:460550 --mem=200GB RQC.sh

#Proportion of sites covered at >5X:

cat quality.txt
#scp dd.pdf to laptop to look at distribution of base quality scores, fraction of sites in each sample passing coverage thresholds and fraction of sites passing genotyping rates cutoffs. Use these to guide choices of -minQ, -minIndDepth and -minInd filters in subsequent ANGSD runs
#Ryan says to use 75-80% for the minIND, so 71 samples for this first round
#minQ just keep at 30, doesn't really matter since we've already filtered
#don't bother with minINDDepth for now

#Identifying clones and technical replicates
FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 30 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 70 -snp_pval 1e-6 -minMaf 0.05"
TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doBcf 1 -doPost 1 -doGlf 2"

echo '#!/bin/bash' > pastClones.sh
echo angsd -b bamsClones -GL 1 $FILTERS $TODO -P 1 -out pastClones >>pastClones.sh

sbatch -o pastClones.o%j -e pastClones.e%j -p shortq7 --mail-type=ALL --mail-user=eshilling2013@fau.edu pastClones.sh

###RUN THIS PART IN R
#Use ibs matrix to identify clones with hierarchical clustering in R. scp to local machine and run chunk below in R

library(tidyverse)
library(magrittr)
library(ggplot2)
library(ggdendro)
cloneBams = read.csv("C:/Users/erin_/Documents/Voss Lab (M.S.)/Lab Work/P.astreoides popgen/poritesastreoidesMetaData.csv")

cloneMa = as.matrix(read.table("C:/Users/erin_/Documents/Voss Lab (M.S.)/Lab Work/P.astreoides popgen/pastClones.ibsMat"))
dimnames(cloneMa) = list(cloneBams[,1],cloneBams[,1])
clonesHc = hclust(as.dist(cloneMa),"ave")
clonePops = cloneBams$region
cloneDend = cloneMa %>% as.dist() %>% hclust(.,"ave") %>% as.dendrogram()
cloneDData = cloneDend %>% dendro_data()

# Making the branches hang shorter so we can easily see clonal groups
cloneDData$segments$yend2 = cloneDData$segments$yend
for(i in 1:nrow(cloneDData$segments)) {
  if (cloneDData$segments$yend2[i] == 0) {
    cloneDData$segments$yend2[i] = (cloneDData$segments$y[i] - 0.01)}}

cloneDendPoints = cloneDData$labels
cloneDendPoints$pop = clonePops[order.dendrogram(cloneDend)]
rownames(cloneDendPoints) = cloneDendPoints$label

# Making points at the leaves to place symbols for populations
point = as.vector(NA)
for(i in 1:nrow(cloneDData$segments)) {
  if (cloneDData$segments$yend[i] == 0) {
    point[i] = cloneDData$segments$y[i] - 0.01
  } else {
    point[i] = NA}}

cloneDendPoints$y = point[!is.na(point)]

techReps = c("P028-1", "P028-2", "P028-3", "P046-1", "P046-2", "P046-3", "P077-1", "P077-2", "P077-3")

cloneDendPoints$pop = factor(cloneDendPoints$pop)
cloneDendPoints$pop = factor(cloneDendPoints$pop,levels(cloneDendPoints$pop)[c(5,3,6,2,4,1)])

cloneDendA = ggplot() +
  geom_segment(data = segment(cloneDData), aes(x = x, y = y, xend = xend, yend = yend2), size = 0.5) +
  geom_point(data = cloneDendPoints, aes(x = x, y = y, fill = pop), size = 4, stroke = 0.25, shape = 24) +
  scale_fill_brewer(palette = "Dark2", name = "Population") +
  geom_hline(yintercept = 0.165, color = "red", lty = 5, size = 0.75) + # creating a dashed line to indicate a clonal distance threshold
  geom_text(data = subset(cloneDendPoints, subset = label %in% techReps), aes(x = x, y = (y - .015), label = label), angle = 90) + # spacing technical replicates further from leaf
  geom_text(data = subset(cloneDendPoints, subset = !label %in% techReps), aes(x = x, y = (y - .010), label = label), angle = 90) +
  labs(y = "Genetic distance (1 - IBS)") +
  theme_classic()

cloneDend = cloneDendA + theme(
  axis.title.x = element_blank(),
  axis.text.x = element_blank(),
  axis.line.x = element_blank(),
  axis.ticks.x = element_blank(),
  axis.title.y = element_text(size = 12, color = "black", angle = 90),
  axis.text.y = element_text(size = 10, color = "black"),
  axis.line.y = element_line(),
  axis.ticks.y = element_line(),
  panel.grid = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  plot.background = element_blank(),
  legend.key = element_blank(),
  legend.title = element_text(size = 12),
  legend.text = element_text(size = 10),
  legend.position = "bottom")

cloneDend

ggsave("C:/Users/erin_/Documents/Voss Lab (M.S.)/Lab Work/P.astreoides popgen/cloneDend.png", plot = cloneDend, height = 8, width = 35, units = "in", dpi = 300)
ggsave("C:/Users/erin_/Documents/Voss Lab (M.S.)/Lab Work/P.astreoides popgen/cloneDend.eps", plot = cloneDend, height = 8, width = 35, units = "in", dpi = 300)

###END OF R DENDROGRAMS

###HERE STARTING BACK IN KOKO
#~~~realized I made a mistake when pipetting my technical replicates vs how I listed them in my lab notebook/excel sheet, fixed in R for dendro but now need to fix names in KOKO~~~
mv P028-3.trim.un.bt2.bam P046-2_fixed.trim.un.bt2.bam
mv P028-3.trim.un.bt2.bam.bai P046-2_fixed.trim.un.bt2.bam.bai
mv P046-2.trim.un.bt2.bam P077-2_fixed.trim.un.bt2.bam
mv P046-2.trim.un.bt2.bam.bai P077-2_fixed.trim.un.bt2.bam.bai
mv P046-3.trim.un.bt2.bam P028-3_fixed.trim.un.bt2.bam
mv P046-3.trim.un.bt2.bam.bai P028-3_fixed.trim.un.bt2.bam.bai
mv P077-2.trim.un.bt2.bam P046-3_fixed.trim.un.bt2.bam
mv P077-2.trim.un.bt2.bam.bai P046-3_fixed.trim.un.bt2.bam.bai

#now renaming the named changed files to remove the "fixed" label
mv P046-2_fixed.trim.un.bt2.bam P046-2_trim.un.bt2.bam
mv P046-2_fixed.trim.un.bt2.bam.bai P046-2_trim.un.bt2.bam.bai
mv P077-2_fixed.trim.un.bt2.bam P077-2_trim.un.bt2.bam
mv P077-2_fixed.trim.un.bt2.bam.bai P077-2_trim.un.bt2.bam.bai
mv P028-3_fixed.trim.un.bt2.bam P028-3_trim.un.bt2.bam
mv P028-3_fixed.trim.un.bt2.bam.bai P028-3_trim.un.bt2.bam.bai
mv P046-3_fixed.trim.un.bt2.bam P046-3_trim.un.bt2.bam
mv P046-3_fixed.trim.un.bt2.bam.bai P046-3_trim.un.bt2.bam.bai

#grouping/concatenating the sequence files that didn't get matched up with their correct barcode
cat P029.trim.un.bt2.bam P1-5_AGGG.trim.un.bt2.bam > P029.trim.un.bt2.bam
cat P029.trim.un.bt2.bam.bai P1-5_AGGG.trim.un.bt2.bam.bai > P029.trim.un.bt2.bai
cat P053.trim.un.bt2.bam P1-5_TCCG.trim.un.bt2.bam > P053.trim.un.bt2.bam
cat P053.trim.un.bt2.bam.bai P1-5_TCCG.trim.un.bt2.bam.bai > P053.trim.un.bt2.bai

#correcting technical replicate labelling in other directories
mv P028-3.tr0 P046-2_fixed.tr0
mv P028-3.trimlog.txt P046-2_fixed_trimlog.txt
mv P046-2.tr0 P077-2_fixed.tr0
mv P046-2.trimlog.txt P077-2_fixed.trimlog.txt
mv P046-3.tr0 P028-3_fixed.tr0
mv P046-3.trimlog.txt P028-3_fixed.trimlog.txt
mv P077-2.tr0 P046-3_fixed.tr0
mv P077-2.trimlog.txt P046-3_fixed.trimlog.txt

mv P046-2_fixed.tr0 P046-2.tr0
mv P046-2_fixed_trimlog.txt P046-2.trimlog.txt
mv P077-2_fixed.tr0 P077-2.tr0
mv P077-2_fixed.trimlog.txt P077-2.trimlog.txt
mv P028-3_fixed.tr0 P028-3.tr0
mv P028-3_fixed.trimlog.txt P028-3.trimlog.txt
mv P046-3_fixed.tr0 P046-3.tr0
mv P046-3_fixed.trimlog.txt P046-3.trimlog.txt

mv P028-3.trim.gz P046-2_fixed.trim.gz
mv P046-2.trim.gz P077-2_fixed.trim.gz
mv P046-3.trim.gz P028-3_fixed.trim.gz
mv P077-2.trim.gz P046-3_fixed.trim.gz

mv P046-2_fixed.trim.gz P046-2.trim.gz
mv P077-2_fixed.trim.gz P077-2.trim.gz
mv P028-3_fixed.trim.gz P028-3.trim.gz
mv P046-3_fixed.trim.gz P046-3.trim.gz

#Removing clones and re-running ANGSD
mkdir clones
mv pastClones* clones

ls *.bam > bamsNoClones

##change to my clones
####and going to have to remove all of my actual samples that turned out to be clones as well
cat bamsClones | grep -v 'P018.trim.un.bt2.bam\|P028-1.trim.un.bt2.bam\|P028-3.trim.un.bt2.bam\|P005.trim.un.bt2.bam\|P012.trim.un.bt2.bam\|P013.trim.un.bt2.bam\|P021.trim.un.bt2.bam\|P023.trim.un.bt2.bam\|P025.trim.un.bt2.bam\|P026.trim.un.bt2.bam\|P030.trim.un.bt2.bam\|P052.trim.un.bt2.bam\|P054.trim.un.bt2.bam\|P059.trim.un.bt2.bam\|P060.trim.un.bt2.bam\|P061.trim.un.bt2.bam\|P062.trim.un.bt2.bam\|P064.trim.un.bt2.bam\|P065.trim.un.bt2.bam\|P066.trim.un.bt2.bam\|P067.trim.un.bt2.bam\|P068.trim.un.bt2.bam\|P069.trim.un.bt2.bam\|P071.trim.un.bt2.bam\|P072.trim.un.bt2.bam\|P075.trim.un.bt2.bam\|P055.trim.un.bt2.bam\|P056.trim.un.bt2.bam\|P058.trim.un.bt2.bam\|P070.trim.un.bt2.bam\|P074.trim.un.bt2.bam\|P076.trim.un.bt2.bam\|P079.trim.un.bt2.bam\|P080.trim.un.bt2.bam\|P081.trim.un.bt2.bam\|P082.trim.un.bt2.bam\|P088.trim.un.bt2.bam\|P089.trim.un.bt2.bam\|P077-1.trim.un.bt2.bam\|P077-2.trim.un.bt2.bam\|P086.trim.un.bt2.bam\|P090.trim.un.bt2.bam\|P046-1.trim.un.bt2.bam\|P046-2.trim.un.bt2.bam\|P046-3.trim.un.bt2.bam\|P047.trim.un.bt2.bam\|P048.trim.un.bt2.bam\|P050.trim.un.bt2.bam\|P051.trim.un.bt2.bam\|P053.trim.un.bt2.bam\|P063.trim.un.bt2.bam\|P036.trim.un.bt2.bam\|P037.trim.un.bt2.bam\|P039.trim.un.bt2.bam\|P042.trim.un.bt2.bam\|P001.trim.un.bt2.bam\|P007.trim.un.bt2.bam' >bamsNoClones

##36 "individuals" left after removal of clones
FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 30 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 27 -snp_pval 1e-6 -minMaf 0.05"
TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doBcf 1 -doPost 1 -doGlf 2"

echo '#!/bin/bash' > pastNoClones.sh
echo srun angsd -b bamsNoClones -GL 1 $FILTERS $TODO -P 1 -out pastNoClones >> pastNoClones.sh

sbatch --mem=200GB -o pastNoClones.o%j -e pastNoClones.e%j -p shortq7 --mail-type=ALL --mail-user=eshilling2013@fau.edu pastNoClones.sh

#How many SNPs?

grep "filtering:" pastNoClones.e*
21,458 SNPs

#H E T E R O Z Y G O S I T Y
#Calculating Heterozygosity across all loci (variant//invariant) using ANGSD and R script from Misha Matz (https://github.com/z0on/2bRAD_denovo)

echo '#!/bin/bash' > RHetVar.sh
echo heterozygosity_beagle.R pastNoClones.beagle.gz >> RHetVar.sh

sbatch -e RHet.e%j -o RHet.o%j --mem=200GB --mail-user eshilling2013@fau.edu --mail-type=ALL RHetVar.sh

cp RHet.e656736 HetSNPs

mkdir angsdPopStats

#Note there are no MAF or snp filters so as not to affect allelic frequencies that may change heterozygosity calculations
FILTERS="-uniqueOnly 1 -remove_bads 1  -skipTriallelic 1 -minMapQ 20 -minQ 30 -doHWE 1 -sb_pval 1e-5 -hetbias_pval 1e-5 -minInd 27"
TODO="-doMajorMinor 1 -doMaf 1 -dosnpstat 1 -doPost 2 -doGeno 11 -doGlf 2"
echo '#!/bin/bash' > pastPopStats.sh
echo srun angsd -b bamsNoClones -GL 1 $FILTERS $TODO -P 1 -out pastPopStats >> pastPopStats.sh
sbatch --mem=200GB -o pastPopStats.o%j -e pastPopStats.e%j -p shortq7 --mail-type=ALL --mail-user=eshilling2013@fau.edu pastPopStats.sh

mv pastPopStats* angsdPopStats/
cd angsdPopStats

#How many sites?
grep "filtering:" pastPopStats.e*
2,227,068 sites

#Calculate heterozygosity
echo '#!/bin/bash' > RHet.sh
echo heterozygosity_beagle.R pastPopStats.beagle.gz >> RHet.sh

sbatch -e RHet.e%j -o RHet.o%j --mem=200GB --mail-user eshilling2013@fau.edu --mail-type=ALL RHet.sh

cp RHet.e656741 HetAllSites

I N B R E E D I N G   &   R E L A T E D N E S S

cd ~/2bRAD/past/sefl/ANGSD

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 30 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 27 -snp_pval 1e-6 -minMaf 0.05"
TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doBcf 1 -doPost 1 -doGlf 3"

echo '#!/bin/bash' > pastNgsRelate.sh
echo srun angsd -b bamsNoClones -GL 1 $FILTERS $TODO -P 1 -out pastNgsRelate >> pastNgsRelate.sh

sbatch --mem=200GB -o pastNgsRelate.o%j -e pastNgsRelate.e%j -p shortq7 --mail-type=ALL --mail-user=eshilling2013@fau.edu pastNgsRelate.sh

zcat pastNgsRelate.mafs.gz | cut -f5 |sed 1d >freq

echo '#!/bin/bash' > ngsRelate.sh
echo ngsRelate -g pastNgsRelate.glf.gz -n 36 -f freq  -O newres >> ngsRelate.sh

sbatch -e ngsRelate.e%j -o ngsRelate.o%j --mem=200GB --mail-user eshilling2013@fau.edu --mail-type=ALL ngsRelate.sh

mkdir ../ngsRelate

mv freq ../ngsRelate/
mv *pastNgs*Relate* ../ngsRelate/
mv newres ../ngsRelate

cd ../ngsRelate

cd ~2bRAD/past/sefl/

mkdir bayescan
cd bayescan

#Note do not use the renamed version, pgdspider has a tough time parsing the samples into pops when you do
cp ../ANGSD/pastNoClones.bcf .
cp ../ANGSD/pastNoClones.vcf .
srun gunzip pastNoClones.vcf.gz

#my koko didn't have this file type, only have .bcf
#convert bcf to vcf.gz
bcftools view -O z -o pastNoClones.vcf.gz pastNoClones.bcf

#trying again
#it didn't want to make a copy of the file so I just unzipped the one, can always convert from bcf again
srun gunzip pastNoClones.vcf.gz

#made a TAB-DELIMITED txt file "seflPastBsPops.txt" with sample bam files & the population they belong to

echo "############
# VCF Parser questions
PARSER_FORMAT=VCF
# Do you want to include a file with population definitions?
VCF_PARSER_POP_QUESTION=true
# Only input following regions (refSeqName:start:end, multiple regions: whitespace separated):
VCF_PARSER_REGION_QUESTION=
# What is the ploidy of the data?
VCF_PARSER_PLOIDY_QUESTION=DIPLOID
# Only output following individuals (ind1, ind2, ind4, ...):
VCF_PARSER_IND_QUESTION=
# Output genotypes as missing if the read depth of a position for the sample is below:
VCF_PARSER_READ_QUESTION=
# Take most likely genotype if "PL" or "GL" is given in the genotype field?
VCF_PARSER_PL_QUESTION=true
# Do you want to exclude loci with only missing data?
VCF_PARSER_EXC_MISSING_LOCI_QUESTION=false
# Select population definition file:
VCF_PARSER_POP_FILE_QUESTION=./seflPastBsPops.txt
# Only output SNPs with a phred-scaled quality of at least:
VCF_PARSER_QUAL_QUESTION=
# Do you want to include non-polymorphic SNPs?
VCF_PARSER_MONOMORPHIC_QUESTION=false
# Output genotypes as missing if the phred-scale genotype quality is below:
VCF_PARSER_GTQUAL_QUESTION=
# GESTE / BayeScan Writer questions
WRITER_FORMAT=GESTE_BAYE_SCAN
# Specify which data type should be included in the GESTE / BayeScan file  (GESTE / BayeScan can only analyze one data type per file):
GESTE_BAYE_SCAN_WRITER_DATA_TYPE_QUESTION=SNP
############" >vcf2bayescan.spid

java -Xmx1024m -Xms512m -jar ~/bin/PGDSpider_2.0.7.1/PGDSpider2-cli.jar -inputfile pastNoClones.bcf -outputfile seflpast.bayescan -spid vcf2bayescan.spid

#Launch BayeScan (Takes 12+ hr depending on read count/SNPs)
echo '#!/bin/bash' > pastBayescan.sh
echo bayescan seflpast.bayescan -threads=100 >> pastBayescan.sh

sbatch -e pastBayescan.e%j -o pastBayescan.o%j -p mediumq7 --mail-user eshilling2013@fau.edu --mail-type=ALL pastBayescan.sh

srun removeBayescanOutliers.pl bayescan=seflpast.baye_fst.txt vcf=pastNoClones.bcf FDR=0.05 mode=extract > seflpastVcfOutliers.vcf

##Using BayeScEnv we can look for outliers realted to depth

echo "21.64 14.67 18.38 3.05 6.96" > depth.txt

cp seflpast.bayescan seflpast.bayeScEnv

echo '#!/bin/bash' > pastBayeScEnv.sh
echo bayescenv seflpast.bayeScEnv -env depth.txt >> pastBayeScEnv.sh

sbatch -e pastBayeScEnv.e%j -o pastBayeScEnv.o%j -p longq7 --mail-user eshilling2013@fau.edu --mail-type=ALL pastBayeScEnv.sh

#Extract outlier SNPs from BayeScEnv output
srun removeBayescanOutliers.pl bayescan=seflpast.bayeS_fst.txt vcf=pastNoClones.bcf FDR=0.05 mode=extract > seflpasttBScEnvVcfOutliers.vcf

##Looking at minor allele frequencies by population Split outlier .vcf by populations
awk '$2 ~ /jupiter/ {print $1}' seflPastBsPops.txt > jup.pop
awk '$2 ~ /westpalm/ {print $1}' seflPastBsPops.txt > wpb.pop
awk '$2 ~ /boynton/ {print $1}' seflPastBsPops.txt > boyn.pop
awk '$2 ~ /stlucie/ {print $1}' seflPastBsPops.txt > slr.pop
awk '$2 ~ /pompano/ {print $1}' seflPastBsPops.txt > pmp.pop

echo '#!/bin/bash' > vcfSplit.sh
for pop in *.pop; do
echo "bcftools view -S $pop seflpastVcfOutliers.vcf > ${pop/.pop/}.vcf">>vcfSplit.sh;
done

sbatch -e vcfSplit.e%j -o vcfSplit.o%j -p shortq7 --mail-user eshilling2013@fau.edu --mail-type=ALL vcfSplit.sh

#Calculate minor allele frequencies
echo '#!/bin/bash' > miaFreq.sh
for file in *.vcf; do
echo "vcftools --vcf $file --freq --out ${file/.vcf/}" >> miaFreq.sh
done

sbatch -e miaFreq.e%j -o miaFreq.o%j -p shortq7 --mail-user eshilling2013@fau.edu --mail-type=ALL miaFreq.sh

##Combine into one file for analysis
rm seflpastVcfOutliers.frq

for file in *.frq; do
awk '{s=(NR==1)?"POP":"\t"FILENAME;$0=$0 OFS s}1' $file > $file.1;
sed 's/.frq*$//g' $file.1 > $file.2;
sed 's/:/\t/g' $file.2 > $file.3;
done

rm *.frq *.frq.1 *.frq.2
for file in *.frq.3; do
mv $file ${file/.3/}
done

awk FNR!=1 *.frq > alleleFreq.txt

echo -e "chrom\tpos\tnAlleles\tnChr\tmajAl\tmajFreq\tminAl\tminFreq\tpop" | cat - alleleFreq.txt > seflpastAlleleFreq.txt

rm alleleFreq.txt

##Looking at minor allele frequencies of inbred individuals in shallow populations
#First split .vcf into inbred and regular samples for each shallow population
for pop in *bred.pop; do
awk '{print $1}' $pop > ${pop/.pop/}.bams;
done

echo '#!/bin/bash' > vcfInbreedSplit.sh
for pop in *.bams; do
echo "bcftools view -S $pop seflpastVcfOutliers.vcf > ${pop/.bams/}.vcf" >> vcfInbreedSplit.sh;
done

sbatch -e vcfInbreedSplit.e%j -o vcfInbreedSplit.o%j -p shortq7 --mail-user eshilling2013@fau.edu --mail-type=ALL vcfInbreedSplit.sh

##Calculate minor allele frequencies
echo '#!/bin/bash' > miaInbredFreq.sh
for file in *bred.vcf; do
echo "vcftools --vcf $file --freq --out ${file/.vcf/}" >> miaInbredFreq.sh
done

sbatch -e miaInbredFreq.e%j -o miaInbredFreq.o%j -p shortq7 --mail-user eshilling2013@fau.edu --mail-type=ALL miaInbredFreq.sh

##Combine into one file for analysis
for file in *bred.frq; do
awk '{s=(NR==1)?"POP":"\t"FILENAME;$0=$0 OFS s}1' $file > $file.1;
sed 's/.frq*$//g' $file.1 > $file.2;
sed 's/:/\t/g' $file.2 > $file.3;
done

rm *.frq *.frq.1 *.frq.2

for file in *.frq.3; do
mv $file ${file/.3/}
done

for file in *Inbred.frq; do
awk '{s=(NR==1)?"INBRED":"\tInbred";$0=$0 OFS s}1' $file > $file.1;
done

for file in *Outbred.frq; do
awk '{s=(NR==1)?"INBRED":"\tOutbred";$0=$0 OFS s}1' $file > $file.1;
done

for file in *.frq.1; do
mv $file ${file/.1/}
done

awk FNR!=1 *bred.frq > alleleFreq.txt

echo -e "chrom\tpos\tnAlleles\tnChr\tmajAl\tmajFreq\tminAl\tminFreq\tpop\tinbred" | cat - alleleFreq.txt > seflpastInbredAlleleFreq.txt

rm alleleFreq.txt

P O P U L A T I O N   S T R U C T U R E
#Calculate population structure from genotype likelihoods using NGSadmix for K from 2 to 11 : FIRST remove all clones/genotyping replicates! (we did this).
##Need to add note here to change to your ANGSD directory where the beagle files are (I added, not on the github page)
cd ../ANGSD
mkdir ../ngsAdmix
cp *beagle* ../ngsAdmix

zipper.py -f bam -a -9 --launcher -e eshilling2013@fau.edu
sbatch zip.slurm

cd ../ngsAdmix

##Create a file with 50 replicate simulations for each value of K 1-8 (num pops + 3)
ngsAdmixLauncher.py -f pastNoClones.beagle.gz --maxK 8 -r 50 -n seflPast --launcher -e eshilling2013@fau.edu

sbatch --mem=200GB seflPastNgsAdmix.slurm

##Calculating most likely value of K
#Next, take the likelihood value from each run of NGSadmix and put them into a file that can be used with Clumpak to calculate the most likely K using the methods of Evanno et al. (2005).
>seflPastNgsAdmixLogfile
for log in seflPast*.log; do
grep -Po 'like=\K[^ ]+' $log >> seflPastNgsAdmixLogfile;
done

##Format for CLUMPAK in R
R

##You are now using R in the terminal
logs <- as.data.frame(read.table("seflPastNgsAdmixLogfile"))

#output is organized with 7, 8 preceding 1, 2, 3 etc.
logs$K <- c(rep("7", 50), rep("8", 50), rep("1", 50), rep("2", 50), rep("3", 50),
rep("4", 50), rep("5", 50), rep("6", 50))
write.table(logs[, c(2, 1)], "seflPastNgsAdmixLogfile_formatted", row.names = F,
        col.names = F, quote = F)
quit()

#No need to save workspace image [press 'n']
n

##Check that your formatted logfile has the appropriate number of entries

cat seflPastNgsAdmixLogfile_formatted | wc -l
##>400?

##make copies of .qopt files to run structure selector on (.Q files)
for file in seflPast*.qopt; do
filename=$(basename -- "$file" .qopt);
cp "$file" "$filename".Q;
done

mkdir seflPastQ
mv seflPast*Q seflPastQ

zip -r seflPastQ.zip seflPastQ

~~~SYMBIONTS~~~

cd ~/2bRAD/past/sefl/
mv mappedReads/symbionts .
cd symbionts

SYMGENOME=~/bin/symGenomes/symbConcatGenome

2bRAD_bowtie2_launcher.py -f zoox -g $SYMGENOME --launcher -e eshilling2013@fau.edu
sbatch --mem=100GB maps.slurm

#turns sam files into bam files
>s2b
for file in *.sam; do
echo "samtools sort -O bam -o ${file/.sam/}.bam $file && samtools index ${file/.sam/}.bam">>s2b;
done
launcher_creator.py -j s2b -n s2b -t 6:00:00 -N 5 -e eshilling2013@fau.edu -q shortq7
sbatch s2b.slurm

#splitting them up by chromosomes
>zooxReads

for i in *.bam; do
echo $i >>zooxReads;
samtools idxstats $i | cut -f 1,3 >>zooxReads;
done
